{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2cff869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20250506)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (45.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfminer.six requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22ac13",
   "metadata": {},
   "source": [
    "# Part 1: Import Libraries & Directory Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "from datetime import date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from pdfminer.high_level import extract_text\n",
    "from io import BytesIO\n",
    "import logging\n",
    "import shutil\n",
    "import threading\n",
    "\n",
    "# Configuration: Define base directory and paths\n",
    "BASE_DIR = os.path.dirname(os.getcwd())  # Parent directory of 'notebooks'\n",
    "PATH_PDF = os.path.join(BASE_DIR, 'PDF', 'Korupsi')  # Where PDFs are stored\n",
    "PATH_OUTPUT = os.path.join(BASE_DIR, 'data', 'raw')    # Where processed text files are saved\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'cleaning.log')\n",
    "\n",
    "# Thread-safe counter for processed files\n",
    "processed_files = 0\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "# Maximum path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "# Validate path length\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist and validate their paths\n",
    "for path in [PATH_PDF, PATH_OUTPUT, LOG_DIR]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Ensure the log directory exists\n",
    "log_dir = \"../logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Initialize logging with write mode to overwrite the log file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir, \"cleaning.log\"), mode='w', encoding='utf-8'),  # Write mode to overwrite\n",
    "    ],\n",
    "    force=True  # Remove existing handlers to avoid conflicts\n",
    ")\n",
    "\n",
    "logging.getLogger().handlers[0].flush()  # Ensure FileHandler flushes immediately\n",
    "logging.info(\"Logging initialized at %s\", LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d03f4",
   "metadata": {},
   "source": [
    "# Part 2: Utility Functions for Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_page(link):\n",
    "    count = 0\n",
    "    while count < 3:\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, \"lxml\")\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            logging.warning(f\"Attempt {count} failed for {link}: {e}\")\n",
    "            time.sleep(5)\n",
    "    logging.error(f\"Could not open {link} after 3 attempts\")\n",
    "    return None\n",
    "\n",
    "def get_pdf_url(soup):\n",
    "    try:\n",
    "        pdf_link = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
    "        if not pdf_link.startswith(\"http\"):\n",
    "            pdf_link = f\"https://putusan3.mahkamahagung.go.id{pdf_link}\"\n",
    "        return pdf_link\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get PDF URL: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_url_already_processed(url, path_pdf):\n",
    "    processed_files_list = [f for f in os.listdir(path_pdf) if f.endswith('.pdf')]\n",
    "    return any(url.split('/')[-1] in f for f in processed_files_list)\n",
    "\n",
    "def download_pdf(url, path_pdf, keyword_url):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(url)\n",
    "        file_name = file.info().get_filename() or url.split('/')[-1]\n",
    "        # Extract a short keyword from keyword_url, default to 'case' if not found\n",
    "        keyword = 'kerugian keuangan negara' if 'kerugian keuangan negara' in keyword_url.lower() else 'case'\n",
    "        # Replace invalid characters and append keyword and date\n",
    "        file_name = re.sub(r'[^\\w\\-_\\.]', '_', file_name.replace(\".pdf\", \"\"))\n",
    "        # Truncate base file name to ensure total path length is safe\n",
    "        max_base_length = MAX_PATH_LENGTH - len(path_pdf) - len(f\"_{keyword}_{date.today().strftime('%Y-%m-%d')}.pdf\") - 10\n",
    "        file_name = file_name[:max_base_length]\n",
    "        file_name = f\"{file_name}_{keyword}_{date.today().strftime('%Y-%m-%d')}.pdf\"\n",
    "        save_path = os.path.join(path_pdf, file_name)\n",
    "        # Final path length check\n",
    "        if len(save_path) > MAX_PATH_LENGTH:\n",
    "            file_name = f\"putusan_{int(time.time())}_{keyword}_{date.today().strftime('%Y-%m-%d')}.pdf\"\n",
    "            save_path = os.path.join(path_pdf, file_name)\n",
    "        file_content = file.read()\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            out_file.write(file_content)\n",
    "        logging.info(f\"Successfully downloaded: {file_name}\")\n",
    "        return file_name\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c5542",
   "metadata": {},
   "source": [
    "# Part 3: Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc59baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(link, keyword_url, max_files):\n",
    "    global processed_files\n",
    "    with file_lock:\n",
    "        if processed_files >= max_files:\n",
    "            logging.info(\"Max file limit reached, stopping extraction\")\n",
    "            return False\n",
    "\n",
    "    if is_url_already_processed(link, PATH_PDF):\n",
    "        logging.info(f\"Skipping duplicate URL: {link}\")\n",
    "        return True\n",
    "\n",
    "    soup = open_page(link)\n",
    "    if not soup:\n",
    "        logging.warning(f\"Failed to open page: {link}\")\n",
    "        return True\n",
    "\n",
    "    link_pdf = get_pdf_url(soup)\n",
    "    if not link_pdf:\n",
    "        logging.info(f\"No PDF found for {link}\")\n",
    "        return True\n",
    "\n",
    "    file_name = download_pdf(link_pdf, PATH_PDF, keyword_url)\n",
    "    if file_name:\n",
    "        with file_lock:\n",
    "            processed_files += 1\n",
    "            logging.info(f\"Processed file {processed_files}/{max_files}: {file_name}\")\n",
    "            if processed_files >= max_files:\n",
    "                return False\n",
    "    else:\n",
    "        logging.info(f\"Continuing extraction despite download failure for {link_pdf}\")\n",
    "        return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966c18d",
   "metadata": {},
   "source": [
    "# Part 4: Page Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd07af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process(keyword_url, page, sort_page, max_files):\n",
    "    global processed_files\n",
    "    with file_lock:\n",
    "        if processed_files >= max_files:\n",
    "            logging.info(\"Max file limit reached, stopping page processing\")\n",
    "            return False\n",
    "\n",
    "    if keyword_url.startswith(\"https\"):\n",
    "        link = f\"{keyword_url}&page={page}\"\n",
    "    else:\n",
    "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
    "    if sort_page:\n",
    "        link = f\"{link}&obf=TANGGAL_PUTUS&obm=desc\"\n",
    "\n",
    "    logging.info(f\"Processing page {page}: {link}\")\n",
    "    soup = open_page(link)\n",
    "    if not soup:\n",
    "        logging.warning(f\"Failed to open page {page}: {link}\")\n",
    "        return False\n",
    "\n",
    "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
    "    for link in links:\n",
    "        with file_lock:\n",
    "            if processed_files >= max_files:\n",
    "                logging.info(\"Max file limit reached during link processing\")\n",
    "                return False\n",
    "        full_link = link[\"href\"]\n",
    "        if not full_link.startswith(\"http\"):\n",
    "            full_link = f\"https://putusan3.mahkamahagung.go.id{full_link}\"\n",
    "        continue_processing = extract_data(full_link, keyword_url, max_files)\n",
    "        if not continue_processing:\n",
    "            logging.info(\"Stopping page processing due to max file limit\")\n",
    "            return False\n",
    "    logging.info(f\"Completed processing page {page}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78106563",
   "metadata": {},
   "source": [
    "# Part 5: Main Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scraper(url=None, max_files=50):\n",
    "    global processed_files\n",
    "    if not url or not url.startswith(\"https://\"):\n",
    "        logging.error(\"Please provide a valid URL\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Starting scraper with URL: {url}, max_files: {max_files}\")\n",
    "    soup = open_page(url)\n",
    "    if not soup:\n",
    "        logging.error(\"Failed to open initial URL, stopping scraper\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not determine last page, defaulting to 1: {e}\")\n",
    "        last_page = 1\n",
    "    logging.info(f\"Scraping {last_page} pages, potential {20 * last_page} files\")\n",
    "\n",
    "    with file_lock:\n",
    "        if processed_files >= max_files:\n",
    "            logging.info(f\"Already have {processed_files} files, max limit reached\")\n",
    "            return\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        futures = []\n",
    "        for page in range(1, last_page + 1):\n",
    "            with file_lock:\n",
    "                if processed_files >= max_files:\n",
    "                    logging.info(\"Max file limit reached, stopping page submission\")\n",
    "                    break\n",
    "            future = executor.submit(run_process, url, page, True, max_files)\n",
    "            futures.append(future)\n",
    "        wait(futures)\n",
    "\n",
    "    with file_lock:\n",
    "        final_count = processed_files\n",
    "    logging.info(f\"Scraping complete. Downloaded {final_count} PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ac5ed",
   "metadata": {},
   "source": [
    "# Part 6: Utility Functions for PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            text = extract_text(BytesIO(file.read()))\n",
    "        logging.info(f\"Extracted text from {os.path.basename(pdf_path)}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        logging.info(\"No text to clean\")\n",
    "        return \"\"\n",
    "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
    "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
    "    text = text.replace(\n",
    "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\", \"\"\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\", \"\"\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\", \"\"\n",
    "    )\n",
    "    text = text.replace(\n",
    "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\", \"\"\n",
    "    )\n",
    "    text = text.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    text = text.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "    text = text.replace(\"Pid.I.A.3\", \"\")\n",
    "    text = text.replace(\"Mahkamah Agung Republik Indonesia\", \"\")\n",
    "    text = re.sub(r'Halaman \\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    original_length = len(re.sub(r'[^a-z0-9]', '', text.replace(\" \", \"\")))\n",
    "    cleaned_length = len(text.replace(\" \", \"\"))\n",
    "    integrity = (cleaned_length / original_length) * 100 if original_length > 0 else 0\n",
    "    if integrity < 80:\n",
    "        logging.warning(f\"Text integrity below 80% for document: {integrity:.1f}%\")\n",
    "        return \"\"\n",
    "    logging.info(f\"Cleaned text successfully, integrity: {integrity:.1f}%\")\n",
    "    return text\n",
    "\n",
    "def save_text_file(text, index, path):\n",
    "    if not text:\n",
    "        logging.info(\"No text to save\")\n",
    "        return None\n",
    "    file_name = f\"case_{index:03d}.txt\"\n",
    "    save_path = os.path.join(path, file_name)\n",
    "    if len(save_path) > MAX_PATH_LENGTH:\n",
    "        logging.error(f\"Output path too long: {save_path}\")\n",
    "        return None\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    logging.info(f\"Saved/overwritten cleaned text: {file_name}\")\n",
    "    return file_name\n",
    "\n",
    "def get_next_index(pdf_files):\n",
    "    return len(pdf_files) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457996b",
   "metadata": {},
   "source": [
    "# Part 7: Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(max_files=50):\n",
    "    logging.info(\"Starting PDF processing with max_files=%d\", max_files)\n",
    "\n",
    "    # Clear existing files in output directory\n",
    "    for file in os.listdir(PATH_OUTPUT):\n",
    "        file_path = os.path.join(PATH_OUTPUT, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            if len(file_path) > MAX_PATH_LENGTH:\n",
    "                logging.warning(f\"Skipping file deletion due to long path: {file_path}\")\n",
    "                continue\n",
    "            os.remove(file_path)\n",
    "            logging.info(f\"Deleted existing file: {file_path}\")\n",
    "    logging.info(f\"Cleared all existing files in %s\", PATH_OUTPUT)\n",
    "\n",
    "    pdf_files_list = [f for f in os.listdir(PATH_PDF) if f.endswith('.pdf')]\n",
    "    logging.info(f\"Found {len(pdf_files_list)} PDF files to process\")\n",
    "\n",
    "    if not pdf_files_list:\n",
    "        logging.info(\"No PDF files found in %s\", PATH_PDF)\n",
    "        print(\"No PDF files found.\")\n",
    "        logging.warning(f\"No PDF files found in {PATH_PDF}\")\n",
    "        return\n",
    "\n",
    "    processed_files_text = 0\n",
    "    for index, pdf_file in enumerate(pdf_files_list, start=1):\n",
    "        if processed_files_text >= max_files:\n",
    "            logging.info(\"Max file limit reached, stopping processing\")\n",
    "            break\n",
    "\n",
    "        pdf_path = os.path.join(PATH_PDF, pdf_file)\n",
    "        logging.info(f\"Started processing PDF: {pdf_file}\")\n",
    "        text = extract_pdf_text(pdf_path)\n",
    "        cleaned_text = clean_text(text)\n",
    "        if cleaned_text:\n",
    "            saved_file = save_text_file(cleaned_text, index, PATH_OUTPUT)\n",
    "            if saved_file:\n",
    "                processed_files_text += 1\n",
    "        else:\n",
    "            logging.info(f\"No valid text after cleaning for {pdf_file}\")\n",
    "\n",
    "    final_count = len([f for f in os.listdir(PATH_OUTPUT) if f.startswith('case_') and f.endswith('.txt')])\n",
    "    logging.info(f\"Processing complete. Generated {final_count} text files.\")\n",
    "    print(f\"Processing completed. Generated {final_count} text files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edaf031",
   "metadata": {},
   "source": [
    "# Part 8: Run the Scraper and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda49234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Generated 50 text files.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logging.info(\"Starting main execution at %s\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    scraper_url = \"https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=9e41907a1cfc34e0b1c265e262d35e44&jd=&tp=&court=&t_put=&t_reg=&t_upl=&t_pr=\"\n",
    "    run_scraper(url=scraper_url, max_files=50)\n",
    "    process_pdfs(max_files=50)\n",
    "    logging.info(\"Main execution completed at %s\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
